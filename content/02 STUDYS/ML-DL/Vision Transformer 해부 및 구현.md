#paper #coding 

> #### '25 ì•„í‚¤í…ì²˜ ìŠ¤í„°ë”” 1ì£¼ì°¨
- vision transformerë¥¼ í•œ ì¤„ì”© í•´ë¶€í•´ë³´ì
- 250403 ì•„í‚¤í…ì²˜ ëŒì•„ê°€ëŠ” ê²ƒê¹Œì§€ í™•ì¸

- [paper link](https://arxiv.org/abs/2010.11929)
- ICLR 2021

---

# Abstract

- í•´ë‹¹ ë…¼ë¬¸ì€ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œ Transforemr êµ¬ì¡°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë²•ì„ ì œì‹œí•˜ë©° íŠ¹íˆ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ ì‚¬ì „ í•™ìŠµëœ Vision Transforer(ViT) ëª¨ë¸ì´ ê¸°ì¡´ì˜ CNN ê¸°ë°˜ ëª¨ë¸ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤ëŠ” ê²ƒì„ ì…ì¦í•œë‹¤.


# Method
- ëª¨ë¸ì€ Transformerë¥¼ ìµœëŒ€í•œ ê°€ê¹ê²Œ í–ˆë‹¤êµ¬ í•¨!
- ë°©ë²•ë¡ ì€ í¬ê²Œ 3ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŒ.
![](https://velog.velcdn.com/images/adsky0309/post/da956404-92aa-410d-a162-82b3825082a9/image.png)


## 1 ì´ë¯¸ì§€ íŒ¨ì¹˜ ë¶„í•  ë° ì„ë² ë”©(Patch Extraction and Embedding)
- ì›ë˜ ì¼ë°˜ì ì¸ TransformerëŠ” í† í° ì„ë² ë”©ì— ëŒ€í•œ 1ì°¨ì›ì˜ ì‹œí€€ìŠ¤ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ!
- ì´ë¯¸ì§€(x)ë¥¼ ê³ ì • í¬ê¸°ì˜ íŒ¨ì¹˜(xp)ë¡œ ë¶„í• 
	ex. 224 X224 ì´ë¯¸ì§€ì—ì„œ 16X16 íŒ¨ì¹˜ ì¶”ì¶œí•˜ë©´ ì´ 196ê°œì˜ íŒ¨ì¹˜ê°€ ìƒê¹€!
- ê° íŒ¨ì¹˜ë¥¼ Flatten
- `H X W X C` -> `N X (P^2 X C)`ë¡œ ë³€í™˜
    - `H` : ì´ë¯¸ì§€ì˜ ë†’ì´(Height)ë¥¼ í”½ì…€ ë‹¨ìœ„ë¡œ
    - `W` : ì´ë¯¸ì§€ì˜ ë„ˆë¹„(width)ë¥¼ í”½ì…€ ë‹¨ìœ„ë¡œ!
    - `C` : ì´ë¯¸ì§€ì˜ ì±„ë„ ìˆ˜. ì»¬ëŸ¬ì´ë¯¸ì§€ëŠ” Red, Green, Blue, ì„¸ ê°œì˜ ì±„ë„ì„ ê°€ì§€ë¯€ë¡œ C=3ì´ ë¨! í‘ë°±ì´ë©´ C=1ê² ì£ ?
    - `(P, P)`ëŠ” ì´ë¯¸ì§€ íŒ¨ì¹˜ì˜ í¬ê¸°
    - N = $$HW/P^2$$ = íŒ¨ì¹˜ì˜ ê°œìˆ˜!!
- ê·¸ë¦¬ê³ ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ì„ í˜• íˆ¬ì˜(linear projection)ì„ ì‚¬ìš©í•´ D ì°¨ì› ë²¡í„°ë¡œ ì„ë² ë”©í•œë‹¤. ì´ ë²¡í„°ë¥¼ íŒ¨ì¹˜ ì„ë² ë”©(Patch Embedding)ì´ë¼ê³  ë¶€ë¦„!!
    - `ì ì¬ ë²¡í„° í¬ê¸°`(Latent Vector Size) D : íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ëª¨ë“  ë ˆì´ì–´ì—ì„œ ì¼ê´€ë˜ê²Œ ì‚¬ìš©ë˜ëŠ” ê³ ì •ëœ í¬ê¸°ì˜ ë²¡í„°. ëª¨ë¸ì˜ ë³µì¡ë„ì™€ í‘œí˜„ë ¥ì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ ìš”ì†Œ!!
    - `ì„ í˜• íˆ¬ì‚¬`(Linear Projection) : í‰íƒ„í™”ëœ íŒ¨ì¹˜ë¥¼ Dì°¨ì›ìœ¼ë¡œ ë§¤í•‘

> ğŸª ViTì—ì„œ Dì°¨ì› ë§¤í•‘í•˜ë©´?
- ì •ë³´ ì••ì¶• : ê³ ì°¨ì› ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ
- íŠ¹ì§• ì¶”ì¶œ : ì´ë¯¸ì§€ì˜ ì¤‘ìš”í•œ ì‹œê°ì  íŠ¹ì§•ì„ íš¨ê³¼ì ìœ¼ë¡œ í‘œí˜„
-> ë°ì´í„°ë¥¼ ë” ë‚®ì€ ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ë˜ ì¤‘ìš”í•œ ì •ë³´ëŠ” ìµœëŒ€í•œ ìœ ì§€í•˜ëŠ” ê³¼ì •!

- BERTì˜ [class] í† í°ê³¼ ìœ ì‚¬í•˜ê²Œ, í•™ìŠµ ê°€ëŠ¥í•œ ë¶„ë¥˜ í† í°(Classification Token)ì„ íŒ¨ì¹˜ ì„ë² ë”© ì‹œí€€ìŠ¤ ì•ì— ì¶”ê°€!

$$
z_0 = [x_{\text{class}}, x^1_p E; x^2_p E; \cdots; x^N_p E] + E_{\text{pos}}, \quad E \in \mathbb{R}^{(P^2 \cdot C) \times D}, \quad E_{\text{pos}} \in \mathbb{R}^{(N+1) \times D}
$$
- ì—¬ê¸°ì„œ $$z_0$$ëŠ” Transformer Encoder ì•ˆì— ë“¤ì–´ê°ˆ ì…ë ¥ê°’ì´ê³  $$x_{class}$$ëŠ” í•™ìŠµì´ ê°€ëŠ¥í•œ classification token, ê·¸ë¦¬ê³  $$x_1pE..$$ëŠ”ì„ë² ë”©ëœ ì´ë¯¸ì§€ íŒ¨ì¹˜ë“¤, $E_{pos}$ëŠ” Position Embedding
- í•µì‹¬ì€ $x_{class}$ê°€ Transformerë¥¼ ê±°ì¹˜ë©´ì„œ ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì„ í•™ìŠµí•˜ê³ , ìµœì¢… ë‹¨ê³„ì—ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ì— ê²°ì •ì ì¸ ì—­í• ì„ í•œë‹¤ëŠ” ì !
- `Classification Token` : Transformerê°€ ì´ë¯¸ì§€ ì „ì²´ì— ëŒ€í•œ ì •ë³´ë¥¼ ì§‘ê³„í•˜ëŠ”ë° ë„ì›€ì„ ì£¼ëŠ” íŠ¹ë³„í•œ í† í°.

## 2. Transformer Encoder
- `Multi-Head Self-Attention(MSA)` ë ˆì´ì–´ì™€ `MLP ë¸”ë¡`ì´ ë²ˆê°ˆì•„ ë‚˜íƒ€ë‚˜ëŠ” êµ¬ì¡°
- ê° ë¸”ë¡ ì „ì— Layer Normalizationì„ ì ìš©í•˜ê³ , ê° ë¸”ë¡ í›„ì—ëŠ” ì”ì°¨ ì—°ê²°(Residual Connection)ì„ ì‚¬ìš©í•¨

$$
z'_l=MSA(LN(z_{l-1}))+z_{l-1}, l=1...L
$$


> #### ğŸª Self-Attention ë‹¤ì‹œ ì •ë¦¬í•˜ê¸°
- ì…ë ¥ëœ ë°ì´í„° ë‚´ì—ì„œ ê° ìš”ì†Œë“¤ì´ ì„œë¡œ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•
- ì´ë¯¸ì§€ì—ì„œëŠ”? -> ê° íŒ¨ì¹˜ë“¤ì´ ì„œë¡œ ì–´ë–»ê²Œ ê´€ë ¨ë˜ì–´ ìˆëŠ”ì§€ íŒŒì•…í•˜ëŠ” ê²ƒ
>
> #### ğŸª Self-Attention ê³„ì‚°ê³¼ì •
- Query(Q) : ë‚´ê°€ ì§€ê¸ˆ ì£¼ëª©í•˜ê³  ìˆëŠ” íŒ¨ì¹˜
- Key(K) : ë¹„êµ ëŒ€ìƒ ë˜ëŠ” ì¹œêµ¬ë“¤
- Value(V) : ì‹¤ì œë¡œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ íŒ¨ì¹˜ë“¤
1. ë¨¼ì € ì…ë ¥ëœ íŒ¨ì¹˜ë“¤ì„ Q, K, Vë¡œ ë³€í™˜
	- ê°ê°ì˜ íŒ¨ì¹˜ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë¥¼ í†µí•´ í–‰ë ¬ë¡œ ë³€í™˜
2. ìœ ì‚¬ë„ ê³„ì‚°
	- Queryì™€ Keyì˜ ë‚´ì (dot product)ì„ ê³„ì‚°!
    - ex. ëˆˆ íŒ¨ì¹˜ì™€ ì½” íŒ¨ì¹˜ëŠ” ê´€ë ¨ì´ ë†’ê² ì¥?
3. Softmax ì ìš©í•´ ì •ê·œí™”
	- ì¤‘ìš”í•œ íŒ¨ì¹˜ì—ëŠ” ë†’ì€ ì ìˆ˜, ëœ ì¤‘ìš”í•œ íŒ¨ì¹˜ì—ëŠ” ë‚®ì€ ì ìˆ˜
4. Valueë¥¼ ê³±í•´ì„œ ìµœì¢… ê²°ê³¼ ì–»ê¸°
	- ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ Value ê°’ë“¤ì„ ëª¨ë‘ ë”í•˜ë©´, ìµœì¢…ì ìœ¼ë¡œ í•´ë‹¹ íŒ¨ì¹˜ì˜ ìƒˆë¡œìš´ í‘œí˜„ì„ ì–»ì„ ìˆ˜ ìˆìŒ!
>
> #### ğŸª MHSAëŠ”?
- ì´ Self-Attentionì„ ì—¬ëŸ¬ ë²ˆ ìˆ˜í–‰í•˜ëŠ” ê²ƒ
- ì™œ ì—¬ëŸ¬ ê°œì˜ Head ì‚¬ìš©? -> í•œ ë²ˆìœ¼ë¡œëŠ” ëª¨ë“  ê´€ê³„ë¥¼ í•œêº¼ë²ˆì— íŒŒì•…í•˜ê¸° ì–´ë ¤ìš°ë¯€ë¡œ ë‹¤ì–‘í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ê¸° ìœ„í•´!

$$
z_l = MLP(LN(z'_l))+z'_l
$$
- MLP ë¸”ë¡ì€ GELU ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” 2ê°œì˜ ë ˆì´ì–´ë¡œ êµ¬ì„±!
- $$z'$$ëŠ” MLP ë¸”ë¡ì˜ ìµœì¢… ì¶œë ¥ìœ¼ë¡œ, ë¨¼ì € LNì„ í•œ í›„ì— MLPë¥¼ ê±°ì¹œë‹¤!

> #### ğŸª Multilayer Perceptron(MLP)
- ì—¬ëŸ¬ ì¸µì˜ í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œ êµ¬ì„±ëœ ì¸ê³µ ì‹ ê²½ë§ì˜ í•œ ì¢…ë¥˜
- ì§ì ‘ì ìœ¼ë¡œ ì—°ê²°í•˜ë©´ ì„ í˜•ì„±ì´ ë„ˆë¬´ ê°•í•˜ê¸° ë•Œë¬¸ì— ì€ë‹‰ì¸µì„ ë°°ì¹˜!
>![](https://velog.velcdn.com/images/adsky0309/post/51dac1d4-d13b-4f4e-8922-8ba328d47997/image.png)
> 

$$
y=LN(z^0_L)
$$

- ë§ˆì§€ë§‰ ì¶œë ¥!
- $y$ëŠ” ìµœì¢… ì´ë¯¸ì§€ í‘œí˜„ìœ¼ë¡œ ëª¨ë¸ì´ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ê²ƒ
- $z_0^L$ : íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ì˜ Lë²ˆì§¸ ë ˆì´ì–´ì—ì„œ Classificationì˜ ì¶œë ¥

![](https://velog.velcdn.com/images/adsky0309/post/388f2b26-7718-4481-a8b1-a09896413344/image.png)

- ë§ˆì§€ë§‰ìœ¼ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸ ìµœì¢… ì¶œë ¥ì—ì„œ [class] í† í°ì˜ ìƒíƒœëŠ” MLP Headë¥¼ í†µê³¼í•´ ìµœì¢… ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ì–»ìŒ!


---
# ì•„í‚¤í…ì²˜ êµ¬í˜„í•´ë³´ê¸°

## Encoder êµ¬í˜„
```python
# Encoder í´ë˜ìŠ¤ êµ¬í˜„í•˜ê¸°
class encoder(nn.Module):
    def __init__(self, embed_size=768, num_heads=3, dropout=0.1): # ë“œë¡­ì•„ì›ƒ : ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ ì¼ë¶€ ë‰´ëŸ°ì˜ ì¶œë ¥ì„ ë¬´ì‘ìœ„ë¡œ 0ìœ¼ë¡œ ë§Œë“œëŠ” ì •ê·œí™” ê¸°ë²• 
        super().__init__()
        self.ln1 = nn.LayerNorm(embed_size)
        self.attention = nn.MultiheadAttention(embed_size, num_heads, dropout, batch_first=True) # ì…ë ¥ í…ì„œ ì°¨ì› ìˆœì„œê°€ (ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, ì„ë² ë”© ì°¨ì›)ì„ì„ ëª…ì‹œ
        self.ln2 = nn.LayerNorm(embed_size)
        self.ff = nn.Sequential(
            nn.Linear(embed_size, 4*embed_size)
            , nn.GELU()
            , nn.Dropout(dropout)
            , nn.Linear(4*embed_size, embed_size)
            , nn.Dropout(dropout)
        )
        self.dropout= nn.Dropout(dropout)
    def forward(self, x):
        x = self.ln1(x)
        x = x + self.attention(x, x, x)[0]
        x = x + self.ff(self.ln2(x))
        return x
```

vision transformerë¶€í„° ì ê¸° ì‹œì‘í•´ì„œ ì—¬ê¸°ëŠ” ê°„ë‹¨í•˜ê²Œ... forward ë¶€ë¶„ë§Œ...
```python
def forward(self, x)
```
- ì‹¤ì œ ê³„ì‚° ê³¼ì •!
- ì—¬ê¸°ì„œ xëŠ” tensor. xì˜ í˜•íƒœëŠ” (ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, ì„ë² ë”© ì°¨ì›)
- në°°ì¹˜ í¬ê¸°ëŠ” í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ë¬¸ì¥ì˜ ê°œìˆ˜. ì‹œí€€ìŠ¤ ê¸¸ì´ëŠ” ê° ë¬¸ì¥(ì‹œí€€ìŠ¤)ë¥¼ êµ¬ì„±í•˜ëŠ” í† í°ì˜ ê°œìˆ˜, ì„ë² ë”© ì°¨ì›ì€ ê° í† í°ì„ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°ì˜ í¬ê¸°(ì—¬ê¸°ì„œëŠ” 768)

```python
x = self.ln1(x)
```
ì²« ë²ˆì§¸ ë ˆì´ì–´ ì •ê·œí™” ì ìš©

```python
x = x + self.attention(x, x, x)[0]
```
- ê° í† í°ì€ ìŠ¤ìŠ¤ë¡œ query, key, valueê°€ ë˜ì–´ ì„œë¡œ ë¹„êµëŒ€ìƒì´ ë¨.
- ë³¸ë˜ multiheadattentionì˜ ë°˜í™˜ê°’ì€ `(ì–´í…ì…˜ ì¶œë ¥, ì–´í…ì…˜ ê°€ì¤‘ì¹˜)`
- xë¥¼ ë”í•˜ëŠ” ê²ƒì€ ì”ì°¨ ì—°ê²°(Residual connection)ì„ ì˜ë¯¸. ì›ë˜ì •ë³´ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ìƒˆë¡­ê²Œ ì–»ì€ ì •ë³´ë¥¼ ì¶”ê°€í•˜ëŠ” ê³¼ì •.

```python
x = x + self.ff(self.ln2(x))
```
`__init__` ì—ì„œ ì—´ì‹¬íˆ ì •ì˜í•œ MLPë¥¼ ln2 ì´í›„ì— ì ìš©ì‹œì¼œì¤ë‹ˆë‹¤.
ë.

---
## Vision Transformer êµ¬í˜„

```python
class VisionTransformer(nn.Module):
    def __init__(self, in_channels=3, num_encoders=6, embed_size=768, img_size=(324, 324), patch_size=16, num_classes=10, num_heads=4):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        num_tokens = (img_size[0]*img_size[1])//(patch_size**2)
        self.class_token = nn.Parameter(torch.randn((embed_size,)), requires_grad=True)
        self.patch_embedding = nn.Linear(in_channels*patch_size**2,embed_size)
        self.pos_embedding = nn.Parameter(torch.randn((num_tokens+1, embed_size)), requires_grad=True)
        self.encoders = nn.ModuleList([
            Encoder(embed_size=embed_size, num_heads=num_heads) for _ in range(num_encoders)
        ])
        self.mlp_head = nn.Sequential(
            nn.LayerNorm(embed_size),
            nn.Linear(embed_size, num_classes)
        )
    def forward(self, x):
        batch_size, channel_size = x.shape[:2]
        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)
        patches = patches.contiguous().view(x.size(0), -1, channel_size*self.patch_size*self.patch_size)
        x = self.patch_embedding(patches)
        class_token = self.class_token.unsqueeze(0).repeat(batch_size, 1, 1)
        x = torch.cat([class_token, x], dim=1)  
        x = x + self.pos_embedding.unsqueeze(0)
        for encoder in self.encoders:
            x = encoder(x)
        x = x[:,0, :].squeeze()
        x = self.mlp_head(x)
        return x
```

í•œ ì¤„ ì”© ë´…ì‹œë‹¹

```python
def __init__(self, in_channels=3, num_encoders=6, embed_size=768, img_size=(324, 324), patch_size=16, num_classes=10, num_heads=4):
```
#### í´ë˜ìŠ¤ ì •ì˜í•˜ê³  ì´ˆê¸°í™” í•¨ìˆ˜ë¥¼ ì‹œì‘!
íŒŒë¼ë¯¸í„°ë¥¼ í•œ ë²ˆ ì‚´í´ë³´ì.
- `in_channels=3`: ì…ë ¥ ì´ë¯¸ì§€ì˜ ì±„ë„ ìˆ˜ (ì˜ˆ: 3ì€ RGB ì»¬ëŸ¬ ì´ë¯¸ì§€)
- `num_encoders=6`: ì‚¬ìš©í•  íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ë¸”ë¡ì˜ ê°œìˆ˜. ëª¨ë¸ì˜ ê¹Šì´ë¥¼ ê²°ì •!
- `embed_size=768`: íŠ¸ëœìŠ¤í¬ë¨¸ ë‚´ë¶€ì—ì„œ ì‚¬ìš©ë  ë²¡í„°(ì„ë² ë”©)ì˜ ì°¨ì›(í¬ê¸°)
- `img_size=(324, 324)`: ì…ë ¥ ì´ë¯¸ì§€ì˜ ë†’ì´ì™€ ë„ˆë¹„ (íŠœí”Œ í˜•íƒœ)
- `patch_size=16`: ì´ë¯¸ì§€ë¥¼ ë‚˜ëˆŒ ì •ì‚¬ê°í˜• íŒ¨ì¹˜ì˜ í•œ ë³€ì˜ í¬ê¸°.
- `num_classes=10`: ìµœì¢…ì ìœ¼ë¡œ ë¶„ë¥˜í•  í´ë˜ìŠ¤ì˜ ê°œìˆ˜ (ì˜ˆ: CIFAR-10 ë°ì´í„°ì…‹ì´ë©´ 10).
- `num_heads=4`: ê° ì¸ì½”ë” ë¸”ë¡ ë‚´ì˜ ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì—ì„œ ì‚¬ìš©í•  í—¤ë“œì˜ ê°œìˆ˜.

ì „ë¶€ ë‚´ê°€ ë°”ê¿€ ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„°ë“¤. ì¼ë‹¨ì€ ê¸°ë³¸ì ìœ¼ë¡œ í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•¨

```python
super().__init__()
```

`nn.Module`ì˜ ì´ˆê¸°í™” í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ê²ƒ. íŒŒì´í† ì¹˜ ëª¨ë¸ì„ ë§Œë“¤ì–´ë´¤ë‹¤ë©´ í•´ë´¤ì„!

```python
self.img_size = img_size
self.patch_size = patch_size
num_tokens = (img_size[0]*img_size[1])//(patch_size**2)
```

ì´ë¯¸ì§€ë‘ íŒ¨ì¹˜ í¬ê¸°ë¥¼ ì €ì¥í•˜ê³ , íŒ¨ì¹˜ ê°œìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤.
img_sizeì˜ [0]ê³¼ [1]ëŠ” ë„ˆë¹„ì™€ ë†’ì´ë‹ˆê¹Œ ê±°ê¸°ì— íŒ¨ì¹˜ ì‚¬ì´ì¦ˆ ì œê³±í•œ ê±¸ ë‚˜ëˆ„ë©´ ê°œìˆ˜ê°€ ë‚˜ì˜¤ì§€ìš”~

#### í´ë˜ìŠ¤ í† í° ì •ì˜!!!!

```python
self.class_token = nn.Parameter(torch.randn((embed_size,)), requires_grad=True)
```

- ViTì˜ í•µì‹¬ ì•„ì´ë””ì–´ ì¤‘ í•˜ë‚˜ì¸ [CLS] í† í°ì„ ì •ì˜í•´ì¤ë‹ˆë‹¤.

ê·¸ëŸ°ë° ê·¸ëƒ¥ ì•„~ í•˜ê³  ë„˜ì–´ê°€ê¸° ì¢€ ê·¸ë˜ì„œ ì €ê¸° ì•ˆì— ìˆëŠ” ê²Œ ë¬´ìŠ¨ ë§ì¸ì§€ ì•Œì•„ë´¤ë‹¤.
- `torch.randn((embed_size,))` : ì£¼ì–´ì§„ embed_sizeì˜ ê¸¸ì´ë¥¼ ê°€ì§€ëŠ” 1ì°¨ì› í…ì„œ(ë²¡í„°)ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒ. randnì„ ì¼ê¸° ë–„ë¬¸ì— ì²˜ìŒì—ëŠ” íŠ¹ë³„í•œ ì˜ë¯¸ê°€ ì—†ëŠ” ëœë¤í•œ ê°’ë“¤ë¡œ ì±„ì›Œì§„ë‹¤. ì•„ë¬´ë˜ë„ ì•ìœ¼ë¡œ ì±„ì›Œì§ˆ ì¹œêµ¬ë‹ˆê¹Œ ì¼ë‹¨ ëœë¤ìœ¼ë¡œ ì±„ìš°ëŠ” ë“¯?
- `nn.Parameter(...)` : torch.randnìœ¼ë¡œ ìƒì„±ëœ ëœë¤ í…ì„œë¥¼ ì´ê±¸ë¡œ ê°ì‹¸ì£¼ë©´ í•´ë‹¹ í…ì„œê°€ **ëª¨ë¸ì˜ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°**ì„ì„ íŒŒì´í† ì¹˜ì—ê²Œ ì•Œë ¤ì£¼ëŠ” ê²ƒ!
- ëª¨ë¸ì´ ë°ì´í„°ë¥¼ í†µí•´ ìŠ¤ìŠ¤ë¡œ ì´ ë²¡í„°ì˜ ì˜ë¯¸ë¥¼ í•™ìŠµí•˜ê²Œ ëœë‹¤êµ¬ í•¨.

> #### ğŸª Class_tokenì´ í•„ìš”í•œ ì´ìœ 
- ViTëŠ” ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ ê°œì˜ íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ê³ , ê° íŒ¨ì¹˜ë¥¼ í•˜ë‚˜ì˜ í† í°ì²˜ëŸ¼ ë‹¤ë£¬ë‹¤. í•˜ì§€ë§Œ ì´ë¯¸ì§€ ì „ì²´ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ì„œëŠ” ì´ ì‹œí€€ìŠ¤ ì •ë³´ë“¤ì„ 'í•˜ë‚˜ì˜ ëŒ€í‘œ ë²¡í„°'ë¡œ ìš”ì•½í•´ì•¼ í•œë‹¤.
- BERTì˜ [CLS] í† í°ì²˜ëŸ¼, ì´ í† í°ì— í•´ë‹¹í•˜ëŠ” ìµœì¢… ì¶œë ¥ ë²¡í„°ê°€ ë¬¸ì¥ ì „ì²´ë¥¼ ëŒ€í‘œ!
- ì‘ë™ë°©ì‹
    - self.class_tokenì„ ì´ë¯¸ì§€ íŒ¨ì¹˜ ì„ë² ë”© ì‹œí€€ìŠ¤ì˜ ë§¨ ì•ì— ì¶”ê°€
    - ì´ì œ ì‹œí€€ìŠ¤ëŠ” (í´ë˜ìŠ¤ í† í°, íŒ¨ì¹˜1 í† í°....) ì´ë ‡ê²Œ ë¨
    - ì „ì²´ ì‹œí€€ìŠ¤ê°€ ì¸ì½”ë”ë“¤ì„ í†µê³¼
    - Self-Attention ë§¤ì»¤ë‹ˆì¦˜ì„ í†µí•´ í´ë˜ìŠ¤ í† í°ì€ ëª¨ë“  íŒ¨ì¹˜ í† í°ë“¤ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ì´ë¯¸ì§€ ì „ì²´ì˜ ì •ë³´ë¥¼ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ë©° ìš”ì•½í•¨(nn.Parameter ì“°ëŠ” ì´ìœ )
    - ëª¨ë“  ì¸ì½”ë” í†µê³¼ í›„, ì‹œí€€ìŠ¤ ë§¨ ì•ì— ìˆë˜ í´ë˜ìŠ¤ í† í°ë§Œ ê°€ì§€ê³  ì™€ì„œ ìµœì¢… ë¶„ë¥˜ê¸°ì— ì…ë ¥


ê°ˆ ê¸¸ì´ ë©€êµ°ìš”. ë‹¤ìŒ!

#### íŒ¨ì¹˜ ì„ë² ë”© ë ˆì´ì–´ ì •ì˜

```python
self.patch_embedding = nn.Linear(in_channels*patch_size**2,embed_size)
```

- ë¬´ìŠ¨ ë§ì´ëƒ í•˜ë©´... íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì€ ì›ë˜ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŒ! ì»´í“¨í„°ëŠ” ì´ë¯¸ì§€ íŒ¨ì¹˜ë¥¼ ë°”ë¡œ ì´í•´í•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì— íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ê³ ì •ëœ í¬ê¸°ì˜ ë²¡í„° í˜•íƒœë¡œ ë°”ê¿”ì¤˜ì•¼ í•œë‹¤.
- ì¦‰ 1) ì´ë¯¸ì§€ë¥¼ ì˜ê²Œ ë‚˜ëˆˆ ê°ê°ì˜ íŒ¨ì¹˜ ì¡°ê°ì„ 2) 1ì°¨ì› ë²¡í„°ë¡œ ì­‰- í¼ì¹œ ë‹¤ìŒ 3) ì„ í˜• ë ˆì´ì–´ì— í†µê³¼ì‹œì¼œì„œ 4) íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ì´í•´í•˜ê³  ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ê³ ì •ëœ í¬ê¸°(embed_size)ì˜ ë²¡í„°(embedding)ë¡œ ë³€í™˜í•˜ëŠ” 5) ì‹ ê²½ë§ ë ˆì´ì–´ë¥¼ ë§Œë“œëŠ” ê²ƒ!
- ì—¬ê¸°ì„œ `patch_size`ëŠ” íŒ¨ì¹˜ í•œ ë³€ì˜ í”½ì…€ ìˆ˜, `in_channels * patch_size**2`ëŠ” íŒ¨ì¹˜ í•˜ë‚˜ë¥¼ 1ì°¨ì›ìœ¼ë¡œ ì­‰ í¼ì³¤ì„ ë•Œì˜ ì´ í”½ì…€ ê°’ ê°œìˆ˜!
- ì˜ˆë¥¼ ë“¤ì–´
    - ê° íŒ¨ì¹˜(16x16 í”½ì…€, 3ê°œ ì±„ë„)ë¥¼ 1ì°¨ì›ìœ¼ë¡œ ì­‰ í¼ì¹˜ë©´(11\*16\*3=768)
    - ì´ ë²¡í„°ë¥¼ ë¯¸ë¦¬ ì •í•´ì§„ embed_size(768ë¡œ ì •í•´ë‘ ) ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€ê²½!

```python
self.pos_embedding = nn.Parameter(torch.randn((num_tokens+1, embed_size)), requires_grad=True)
```

ê° í† í°ì˜ ìˆœì„œ ë˜ëŠ” ìœ„ì¹˜ ì •ë³´ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•œ íŒŒë¼ë¯¸í„°!

- `torch.randn` : í…ì„œ ìƒì„±
- `num_tokens+1` : +1ì´ ì˜ë¯¸í•˜ëŠ” ê±´ í´ë˜ìŠ¤ í† í°ì´ ìˆì–´ì•¼ í•˜ê¸° ë•Œë¬¸
- `embed_size` : ê° ìœ„ì¹˜ ì •ë³´ë¥¼ ë‚˜íƒ€ë‚¼ ë²¡í„°ì˜ í¬ê¸°. `íŒ¨ì¹˜ ì„ë² ë”© í¬ê¸°`ë‘ ë™ì¼í•´ì•¼ í•¨! ì™œëƒë©´ ë‚˜ì¤‘ì— ë”í•´ì¤„ ê²ƒì´ê¸° ë•Œë¬¸ì´ì£ ~~
- ë‚˜ë¨¸ì§€ëŠ” `class_token` í•´ì¤¬ë˜ ê±°ë‘ ë¹„ìŠ·

#### ì¸ì½”ë” ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ ìƒì„±

```python
self.encoders = nn.ModuleList([
            Encoder(embed_size=embed_size, num_heads=num_heads) for _ in range(num_encoders)
        ])
```

ìš°ë¦¬ê°€ ì•ì—ì„œ ì •ì˜í•œ `Encoder` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•´ì„œ `num_encoder` ë§Œí¼ ì¸ì½”ë” ë¸”ë¡ì„ í˜•ì„±í•´ì•¼ í•¨! ì´ˆê¸°ì— 6ìœ¼ë¡œ ì •ì˜í–ˆì—ˆìŒ!

- `nn.ModuleList` : íŒŒì´í† ì¹˜ ëª¨ë“ˆë“¤ì„ ë‹´ëŠ” ë¦¬ìŠ¤íŠ¸ì™€ ìœ ì‚¬í•œ ì»¨í…Œì´ë„ˆ

#### ìµœì¢… ë¶„ë¥˜ë¥¼ ìœ„í•œ MLP í—¤ë“œ ì •ì˜

```python
self.mlp_head = nn.Sequential(
            nn.LayerNorm(embed_size),
            nn.Linear(embed_size, num_classes)
        )
```

- í´ë˜ìŠ¤ í† í°ì˜ ìµœì¢… ë²¡í„°ë¥¼ ë°›ì•„ì„œ ìµœì¢… í´ë˜ìŠ¤ ì˜ˆì¸¡ í™•ë¥ ì„ ì¶œë ¥í•˜ëŠ” ë¶€ë¶„!
- `nn.Sequential`ì€ ì—¬ëŸ¬ ë ˆì´ëŸ¬ë¥¼ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ëŠ” ì»¨í…Œì´ë„ˆ!
- ë²¡í„°ë¥¼ ë¨¼ì € ì •ê·œí™”í•œ í›„ì— `embed_size` ì°¨ì›ì˜ ë²¡í„°ë¥¼ ì…ë ¥ë°›ì•„ `num_classes` ê°œìˆ˜ì˜ í´ë˜ìŠ¤ ì ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” ìµœì¢… ì„ í˜• ë ˆì´ì–´!!

---
ì´ì œ ìˆœì „íŒŒ í•¨ìˆ˜ ì •ì˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.

```python
def forward(self, x):
```
ì…ë ¥ xëŠ” ë³´í†µ (ë°°ì¹˜ í¬ê¸°, ì±„ë„ ìˆ˜, ë†’ì´, ë„ˆë¹„) í˜•íƒœì˜ ì´ë¯¸ì§€ í…ì„œ

```python
batch_size, channel_size = x.shape[:2]
```
ìœ„ì—ì„œ ë§í–ˆë“¯... ë°°ì¹˜ í¬ê¸°ì™€ ì±„ë„ ìˆ˜ë¥¼ ê°€ì§€ê³  ì™€ì£¼ê¸°

```python
patches=x.unfold(2,self.patch_size, self.patch_size).unfold(3, self.patch_size, patch_size)
```
ì£¼ì–´ì§„ ì´ë¯¸ì§€ xë¥¼ íŒ¨ì¹˜í™”
- unfold(2, ...)ëŠ” ë†’ì´(H) ì°¨ì›ì„ ë”°ë¼ `self.patch_size` ê°„ê²©ìœ¼ë¡œ ì˜ë¼ë‚´ê¸°
- unfold(3, ...)ëŠ” ë„ˆë¹„(W) ì°¨ì›ì„ ë”°ë¼ ì˜ë¼ë‚¸ ê²ƒ

ì¢€ ë” ìƒì„¸íˆ ì„¤ëª…í•˜ë©´

- ì¼ë‹¨ unfoldë¼ëŠ” í•¨ìˆ˜ëŠ” `unfold(dimension, size, step)`ì¸ ê²ƒ!
- ì²« unfoldëŠ” ì§€ì •ëœ ì°¨ì›(=dimension)ì„ ë”°ë¼ P í¬ê¸°(=size)ì˜ ì¡°ê°ë“¤ì„ Pê°„ê²©(=step)ìœ¼ë¡œ ì˜ë¼ë‚´ë‚´ê³  ì´ ì¡°ê°ë“¤ì€ ìƒˆë¡œìš´ ì°¨ì›ìœ¼ë¡œ ìŒ“ì¸ë‹¤.
- ê·¸ëŸ¼ ì›ë˜ì˜ H ì°¨ì›ì€ ë‘ ê°œì˜ ì°¨ì›ìœ¼ë¡œ ë‚˜ë‰œë‹¤.
    - íŒ¨ì¹˜ì˜ ê°œìˆ˜(=`num_patches_h`) : ë†’ì´ Hë¥¼ P í¬ê¸°ë¡œ ëª‡ ë²ˆ ì˜ë¼ë‚¼ ìˆ˜ ìˆëŠ”ì§€. H//P
    - ê° íŒ¨ì¹˜ì˜ ë†’ì´(= `patch_height`) : ì˜ë¼ë‚¸ ê° ì¡°ê°ì˜ í¬ê¸° P
    - unfoldëŠ” ìƒˆë¡œìš´ ì°¨ì› `patch_height=P` ë¥¼ í…ì„œ ë§¨ ë§ˆì§€ë§‰ì— ì¶”ê°€í•¨
- ê²°ê³¼ : `(B, C, num_patches_h, W, P)`
- ë‘ ë²ˆì¨° unfoldë„ í•œë‹¤ë©´?
- ê²°ê³¼ : `(B, C, num_patches_h, num_patches_w, P, P)`
	= `(ë°°ì¹˜ í¬ê¸°, ì±„ë„ ìˆ˜, íŒ¨ì¹˜ ê°œìˆ˜(ë†’ì´), íŒ¨ì¹˜ ê°œìˆ˜(ë„ˆë¹„), íŒ¨ì¹˜ ë†’ì´, íŒ¨ì¹˜ ë„ˆë¹„)`
 
#### íŒ¨ì¹˜ í…ì„œ í˜•íƒœ ë³€ê²½

```python
patches = patches.contiguous().view(x.size(0), -1, channel_size*self.patch_size*self.patch_size)
```

- unfoldë¡œ ì–»ì€ íŒ¨ì¹˜ë“¤ì„ íŠ¸ëœìŠ¤í¬ë¨¸ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ `(ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, íŠ¹ì§• ì°¨ì›)` í˜•íƒœë¡œ ë³€ê²½!
- ì´ ì¹œêµ¬ë¥¼ ViTì˜ ë§¥ë½ì— ë§ê²Œ ë°”ê¿”ë³´ë©´ `(ë°°ì¹˜ í¬ê¸°, ì´ íŒ¨ì¹˜ì˜ ê°œìˆ˜, ê° íŒ¨ì¹˜ì˜ íŠ¹ì§• ë²¡í„° ì°¨ì›)`ì´ ëœë‹¤.
- `contiguous()` : ë¨¼ì € ë©”ëª¨ë¦¬ ìƒì—ì„œ ì¡°ê°ë“¤ì„ ì—°ì†ì ìœ¼ë¡œ ë°°ì¹˜
- `view` : í…ì„œì˜ í˜•íƒœë¥¼ ì¬êµ¬ì„±í•˜ëŠ” ê²ƒ
- `x.size(0)` : ë°°ì¹˜ í¬ê¸°ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³ 
- `-1` ë‘ ë²ˆì§¸ ì°¨ì›ì¸ ì‹œí€€ìŠ¤ ê¸¸ì´(=íŒ¨ì¹˜ ê°œìˆ˜)ëŠ” ìë™ìœ¼ë¡œ ê³„ì‚°(=íŒ¨ì¹˜ ê°œìˆ˜(ë†’ì´)*íŒ¨ì¹˜ ê°œìˆ˜(ë„ˆë¹„))
    - view í•¨ìˆ˜ì—ì„œ ì°¨ì› í¬ê¸°ë¥¼ -1ë¡œ ì§€ì •í•˜ë©´ íŒŒì´í† ì¹˜ê°€ 'ë‚˜ë¨¸ì§€ ì›ì†Œë“¤ì„ ëª¨ë‘ ì—¬ê¸°ì„œ ë§ì¶°ì„œ ì•Œì•„ì„œ ê³„ì‚°í•´ì¤˜'ë¼ê³  ì´í•´í•œë‹¤.
    - viewëŠ” ì „ì²´ ì›ì†Œ ê°œìˆ˜(B)ì™€ ë‹¤ë¥¸ ì°¨ì›ë“¤ì˜ í¬ê¸°(C\*P\*P)ë¥¼ ë³´ê³  ë‘ ë²ˆì§¸ ì°¨ì›ì˜ í¬ê¸°ê°€ `num_patches_h*num_patches_w`ë¼ëŠ” ê²ƒì„ ìë™ìœ¼ë¡œ ê³„ì‚°!!! ëŒ€ë°•.
- `channel_size*self.patch_size*self.patch_size` : í•˜ë‚˜ì˜ íŒ¨ì¹˜ ì•ˆì— ë“¤ì–´ìˆëŠ” ëª¨ë“  ì •ë³´ì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. íŒ¨ì¹˜ í•˜ë‚˜ë¥¼ ì™„ì „íˆ í¼ì¹˜ë©´ C\*P\*Pê°€ ëœë‹¤.

```python
x = self.patch_embedding(patches)
```

- 1ì°¨ì›ìœ¼ë¡œ í¼ì¹œ patch ë²¡í„°ë¥¼ patch_embedding ì„ í˜• ë ˆì´ì–´ì— ë„£ì–´ì£¼ë©´ embed_size ì°¨ì›ì˜ ì„ë² ë”© ë²¡í„° ì™„ì„±
- xì˜ í˜•íƒœëŠ” `(ë°°ì¹˜ í¬ê¸°, ì±„ë„ ìˆ˜, ë†’ì´, ë„ˆë¹„)` -> `(ë°°ì¹˜ í¬ê¸°, íŒ¨ì¹˜ ê°œìˆ˜, embed_size)`

```python
class_token = self.class_token.unsqueeze(0).repeat(batch_size, 1, 1)
```

ê°ê°ì˜ ì´ë¯¸ì§€ íŒ¨ì¹˜ ì‹œí€€ìŠ¤ ë§¨ ì•ì— self.class_tokenì´ë¼ëŠ” ë²¡í„°ë¥¼ í•˜ë‚˜ì”© ì¶”ê°€í•´ì•¼ í•œë‹¤. x í…ì„œì™€ í•©ì¹˜ê¸° ìœ„í•´ì„œëŠ”(concat) class_tokenì„ `(ë°°ì¹˜ í¬ê¸°, 1, embed_size)`ë¡œ ë§ì¶°ì•¼ í•œë‹¤. (1ì¸ ì´ìœ ëŠ” í•˜ë‚˜ì˜ ìœ„ì¹˜ë§Œ ì°¨ì§€í•˜ë‹ˆê¹Œ)
- self.class_tokenì˜ ì‹œì‘ í˜•íƒœëŠ” (embed_size, ) -> `(1,embed_size)`
    - `unsqueeze(0)` : í…ì„œì˜ 0ë²ˆì§¸ ìœ„ì¹˜(ë§¨ ì•)ì— í¬ê¸°ê°€ 1ì¸ ìƒˆë¡œìš´ ì°¨ì›ì„ ì¶”ê°€
- `repeat` í•¨ìˆ˜ëŠ” ê° ì°¨ì›ì„ ëª‡ ë²ˆ ë°˜ë³µí• ì§€ ì•Œë ¤ì¤Œ
    - ê°€ìƒì˜ (1, 1, embed_size)ì— repeat(batch_size, 1, 1)ì„ ì ìš©
- ìµœì¢… ê²°ê³¼
    - `(batch_size, 1, embed_size)` ex. (B, 1, 768)
    - ì˜ë¯¸ : batch_size ê°œì˜ ë™ì¼í•œ í´ë˜ìŠ¤ í† í°ì´ ë§Œë“¤ì–´ì¡Œê³ , ê° í† í°ì€ ì‹œí€€ìŠ¤ ìƒì—ì„œ í•˜ë‚˜ì˜ ìœ„ì¹˜(í¬ê¸° 1)ë¥¼ ì°¨ì§€í•˜ë©°, ê°ê° embed_size ì°¨ì›ì˜ ë²¡í„°ë¥¼ ê°€ì§.
    
> #### ğŸª repeat íŠ¹ì§•
- input í…ì„œê°€ 2ì°¨ì›ì¸ë° repeat ì—ëŠ” 3ê°œì˜ ì¸ì, 3ì°¨ì›ì¼ ê²½ìš°
- íŒŒì´í† ì¹˜ëŠ” "ì‚¬ìš©ìê°€ ë” ë†’ì€ ì°¨ì›ì„ ì›í•˜ëŠ”êµ°"ì´ë¼ê³  ìƒê°í•˜ê³  ì…ë ¥ í…ì„œì˜ ë§¨ ì•ì— í¬ê¸°ê°€ 1ì¸ ì°¨ì›ì„ í•„ìš”í•œ ë§Œí¼ ìë™ìœ¼ë¡œ ì¶”ê°€í•´ì„œ ì ìš©!


```python
x=torch.cat([class_token, x], dim=1)
```
ì¤€ë¹„í•œ í´ë˜ìŠ¤ í† í°ê³¼ xë¥¼ ì´ì–´ë¶™ì—¬ì¤ë‹ˆë‹¤~
ì´ë•Œ ì˜ ë¶™ì—¬ì§€ë¼ê³  ì•ì—ì„œ ê·¸ëŸ° ê³ ìƒì„ í•œ ê²ƒ..
- ê²°ê³¼ : `(batch_size, íŒ¨ì¹˜ ê°œìˆ˜ +1, embed_size)`

#### ìœ„ì¹˜ ì„ë² ë”© ì¶”ê°€!

```python
x = x+self.pos_embedding.unsqueeze(0)
```

- ê° í† í°(ì´ì œëŠ” í´ë˜ìŠ¤+íŒ¨ì¹˜ í† í°)ì— í•´ë‹¹ ìœ„ì¹˜ ì„ë² ë”©ì„ ë”í•´ì¤€ë‹¤
- self.pos_embedding : `(íŒ¨ì¹˜ ê°œìˆ˜+1, embed_size)` í˜•íƒœì˜ íŒŒë¼ë¯¸í„°(\__init__ì—ì„œ ê·¸ë ‡ê²Œ ì§€ì •í•´ì¤Œ!)
- unsqueeze(0) : ì´ê±¸ë¡œ `(1, íŒ¨ì¹˜ ê°œìˆ˜+1, embed_size)`ê°€ ë¨
- ì•ì—ì„œ ë§Œë“  xëŠ” ë§í–ˆë“¯ì´ `(batch_size, íŒ¨ì¹˜ ê°œìˆ˜+1, embed_size)`
- ë‘ ê°œë¥¼ ë”í•˜ë©´ `(batch_size, íŒ¨ì¹˜ ê°œìˆ˜+1, embed_size)`

#### ì´ì œ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” í†µê³¼!!
```python
for encoder in self.encoders:
	x=encoder(x)
```

- `self.encoders` ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ê° ì¸ì½”ë” ë¶ˆë¡ì„ ìˆœì„œëŒ€ë¡œ í†µê³¼
    - encoder í´ë˜ìŠ¤ë¥¼ ëŒë¦¬ëŠ” ê²ƒ
- ëª¨ë“  ì¸ì½”ë”ë¥¼ í†µê³¼ì‹œí‚¨ í›„ì—, ìµœì¢… ë¶„ë¥˜ë¥¼ ìœ„í•´ ì‹œí€€ìŠ¤ì˜ ë§¨ ì•ì— ìˆë˜ í´ë˜ìŠ¤ í† í°([CLS])ì˜ ìµœì¢… ë°±í„°ë§Œ ì¶”ì¶œ!

```python
  x=x[:,0,:].squeeze()
  x=self.mlp_head(x)
return x
```
- `x[:,0,:]` : ëª¨ë“  ë°°ì¹˜ ìƒ˜í”Œì— ëŒ€í•´ ì‹œí€€ìŠ¤ì˜ 0ë²ˆì§¸ ì¸ë±ìŠ¤ì˜ ëª¨ë“  ì„ë² ë”© ì°¨ì›ì„ ì„ íƒ
- `squeeze()` : ì›ë˜ (ë°°ì¹˜ í¬ê¸°, 1, embed_size)ì˜€ëŠ”ë° 1ì¸ ì°¨ì›ì„ ì œê±°í•´ì„œ (ë°°ì¹˜ í¬ê¸°, embed_size)ê°€ ë¨.
- ë§ˆì§€ë§‰ìœ¼ë¡œ MLP í—¤ë“œë¥¼ í†µê³¼í•´ì„œ ìµœì¢… ë¶„ë¥˜
- ê²°ê³¼ xì˜ í˜•íƒœ : `(ë°°ì¹˜ í¬ê¸°, num_classes)`


# Testí•˜ê¸°

```python
from torchinfo import summary
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"device : {device}")
model = VisionTransformer(in_channels=1, img_size=(28, 28), patch_size=7, embed_size=64, num_heads=4, num_encoders=3).to(device)
summary(model, [2, 1, 28, 28])
```


![](https://velog.velcdn.com/images/adsky0309/post/4c924cc5-814b-4be3-b345-0186d5d7b43f/image.png)

ì•”íŠ¼ ì˜ ëŒì•„ê°„ë‹¤!!
ì•„í‚¤í…ì²˜ ìì²´ëŠ” ê´œì°®ë‹¤!!!! (ã… ã……ã…¡)

# í•™ìŠµ & í‰ê°€

> ğŸ›  ì•„ì§ ëšë”±ëšë”±... ğŸ› 

## í•™ìŠµ ë‹¨ê³„

1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (torch, torchvision, torchinfo ë“±)
2. Encoder ë° VisionTransformer í´ë˜ìŠ¤ ì •ì˜ (ì´ì „ ì½”ë“œ ì‚¬ìš©)
3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸°, ì—í­ ìˆ˜ ë“±)
4. ì¥ì¹˜ ì„¤ì • (GPU ìš°ì„  ì‚¬ìš©)
5. MNIST ë°ì´í„°ì…‹ ë¡œë“œ ë° ë³€í™˜ (Transform) ì •ì˜
6. ë°ì´í„° ë¡œë”(DataLoader) ìƒì„±
7. ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (MNISTì— ë§ëŠ” íŒŒë¼ë¯¸í„° ì‚¬ìš©) ë° ì¥ì¹˜ë¡œ ì´ë™
8. ì†ì‹¤ í•¨ìˆ˜(Loss Function) ë° ì˜µí‹°ë§ˆì´ì €(Optimizer) ì •ì˜
9. í›ˆë ¨ ë£¨í”„(Training Loop) ì •ì˜
10. í‰ê°€ ë£¨í”„(Evaluation Loop) ì •ì˜
11. í›ˆë ¨ ë° í‰ê°€ ì‹¤í–‰

ëª¨ë¸ ë§Œë“œëŠ”ê²Œ ëì´ ì•„ë‹ˆë¼ëŠ” ì´ ì ˆë§ê°... ì²˜ìŒì´ í˜ë“ ê±°ë¼ê³  ìƒê°í•˜ê³  ë§ˆìŒì„ ë‹¤ì¡ìŒ...

### í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸

```python

import torch
import torchvision
import torch.nn as nn
import torchvision.transforms as transforms
import torch.optim as optim
import torchvision.datasets as datasets
from torch.utils.data import DataLoader
import time
```

### í´ë˜ìŠ¤ ì •ì˜

```python
from ViT import encoder, VisionTransformer
```

ë‹¤ë¥¸ íŒŒì¼ì— ë§Œë“¤ì–´ë’€ê¸° ë•Œë¬¸ì— importë¥¼ ì‹œì¼œì¤ë‹ˆë‹¤.

### í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¤ì •

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using Device : {device}")

# ViT íŒŒë¼ë¯¸í„° ì„¤ì •í•˜ê¸°
img_size = (28, 28)
patch_size = 7
in_channel = 1 #MNIST í‘ë°±
num_classes = 10 # MINST 0~9
embed_size = 128
num_head = 4
num_encoders = 3
dropout =0.1

# í•™ìŠµ íŒŒë¼ë¯¸í„°
batch_size = 128
learning_rate = 1e-4
epochs = 10 
```
MNIST ë°ì´í„°ì…‹ì— ë§ì¶°ì¤€ë‹¤....ì´ëŸ° ë°ì´í„°ì…‹ì¸ì§€ëŠ” Geminiê°€ ì•Œë ¤ì¤Œ.

### MNIST ë°ì´í„°ì…‹ ë° ë°˜í™˜

```python
transform = transforms.Compose([
    transforms.ToTensor(), # ì´ë¯¸ì§€ë¥¼ Tensorë¡œ ë³€í™˜í•˜ê³  [0, 1] ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§
    transforms.Normalize((0.1307,), (0.3081,)) # MNIST í‰ê·  ë° í‘œì¤€í¸ì°¨ë¡œ ì •ê·œí™”
])

train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)
```

### ë°ì´í„° ë¡œë”

```python
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)
```

ë°ì´í„°ì…‹ ê°€ì§€ê³  ì™”ëŠ”ë° ë¡œë”ëŠ” ë¬´ì—‡ì´ëƒ...
datasets. ì€ MNIST ë°ì´í„°ì…‹ ì „ì²´ë¥¼ ë¶ˆëŸ¬ì™€ì„œ íŒŒì´ì¬ ê°ì²´ë¡œ ë§Œë“  ê²ƒ.
DataLoaderê°€ ìˆìœ¼ë©´ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´ í•„ìš”í•œ ì¶”ê°€ì ì¸ ìš”êµ¬ ì‚¬í•­ì„  ìë™í™”í•´ì¤Œ.

- ë°°ì¹˜, ì…”í”Œë§, ë³‘ë ¬ ì²˜ë¦¬, ë©”ëª¨ë¦¬ ê³ ì • ë“±ì˜ íš¨ê³¼ë¥¼ ëˆ„ë¦´ ìˆ˜ ìˆë‹¤~~

### ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°

```python
model = VisionTransformer(
    in_channels=IN_CHANNELS,
    num_encoders=NUM_ENCODERS,
    embed_size=EMBED_SIZE,
    img_size=IMG_SIZE,
    patch_size=PATCH_SIZE,
    num_classes=NUM_CLASSES,
    num_heads=NUM_HEADS,
    dropout=DROPOUT
).to(DEVICE)
```
ê·¸ë¦¬ê³  criterion ê³¼ optimizerë¥¼ ì •ì˜í•œë‹¤.
```python
criterion = nn.CrossEntropyLoss()
```
- ëª©ì : ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ ì‹¤ì œ ì •ë‹µê³¼ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ ì¸¡ì •í•˜ëŠ” ê¸°ì¤€ì„ ì •ì˜
- ëª¨ë¸ì´ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ë³´ê³  í´ë˜ìŠ¤ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ì¶œë ¥í•˜ë©´ CrossEntropyLossëŠ” ëª¨ë¸ì˜ ì¶œë ¥ ì ìˆ˜ì™€ ì‹¤ì œ ì •ë‹µ ë ˆì´ë¸”ì„ ë¹„êµí•œë‹¤.
- ë‚´ë¶€ì ìœ¼ë¡œëŠ” ëª¨ë¸ì˜ ì ìˆ˜ë¥¼ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜(Softmaz)í•˜ê³ , ì‹¤ì œ ì •ë‹µ ë ˆì´ë¸”ê³¼ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•´ì„œ ì†ì‹¤ê°’(loss)ë¡œ ë°˜í™˜

```python
optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)
```
- ìœ„ì—ì„œ ê³„ì‚°ëœ loss ê°’ì„ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ëª¨ë¸ ë‚´ë¶€ì˜ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì–´ë–»ê²Œ ì—…ë°ì´íŠ¸í• ì§€ ë°©ë²•ì„ ì •ì˜!
- `loss.backward()` : ì†ì‹¤ê°’ì„ ê³„ì‚°í•œ í›„ ì´ ë©”ì†Œë“œë¥¼ í˜¸ì¶œ
- `optimizer.step()` : íŒŒë¼ë¯¸í„°ê°’ì„ ì¡°ê¸ˆì”© ì—…ë°ì´íŠ¸
- `model.parameters()` : ì˜µí‹°ë§ˆì´ì €ì—ê²Œ "ì´ ëª¨ë¸  ì•ˆì— ìˆëŠ” íŒŒë¼ë¯¸í„° ë„¤ê°€ ì•Œì•„ì„œ ê´€ë¦¬í•´"ë¼ê³  í•˜ëŠ” ê²ƒ
- `LEARNING_RATE` : ì–¼ë§ˆë‚˜ í° í­ìœ¼ë¡œ ìˆ˜ì •í• ì§€~~

> ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì€ ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ê³ (=forward), ì„±ì ì„ ë§¤ê¸°ê³ (=criterion) -> ì˜¤ë‹µ ë…¸íŠ¸ë¥¼ ë§Œë“¤ê³ (loss.backward()) ->  ê·¸ê±¸ ë³´ê³  ê°œì„ (=optimizer.step())í•˜ëŠ” ê²ƒì˜ ë°˜ë³µ!

### ë“œë””ì–´ í›ˆë ¨

> 0403 ì—¬ê¸°ì„œ ë§‰í˜...

```python
def train_one_epoch(model, loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0

    start_time = time.time()
    for i, (images, labels) in enumerate(loader):
        images = images.to(device)
        labels = labels.to(device)

        # ìˆœì „íŒŒ
        outputs = model(images)
        loss = criterion(outputs, labels)

        # ì—­ì „íŒŒ ë° ìµœì í™”
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # í†µê³„ ì—…ë°ì´íŠ¸
        running_loss += loss.item() * images.size(0)
        _, predicted = torch.max(outputs.data, 1)
        total_samples += labels.size(0)
        correct_predictions += (predicted == labels).sum().item()

        # ì§„í–‰ ìƒí™© ì¶œë ¥ (ì„ íƒ ì‚¬í•­)
        if (i + 1) % 100 == 0:
             print(f"  Batch {i+1}/{len(loader)}, Loss: {loss.item():.4f}")

    epoch_loss = running_loss / total_samples
    epoch_acc = correct_predictions / total_samples
    epoch_time = time.time() - start_time
    print(f"Epoch Training Time: {epoch_time:.2f}s")
    return epoch_loss, epoch_acc
```


## í‰ê°€ ë‹¨ê³„

```python
def evaluate(model, loader, criterion, device):
    model.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0

    with torch.no_grad(): # ê¸°ìš¸ê¸° ê³„ì‚° ë¹„í™œì„±í™”
        for images, labels in loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item() * images.size(0)
            _, predicted = torch.max(outputs.data, 1)
            total_samples += labels.size(0)
            correct_predictions += (predicted == labels).sum().item()

    epoch_loss = running_loss / total_samples
    epoch_acc = correct_predictions / total_samples
    return epoch_loss, epoch_acc
```

### í›ˆë ¨ ë° í‰ê°€ë¥¼ ì‹¤í–‰
```python
print("Starting Training...")
for epoch in range(EPOCHS):
    print(f"--- Epoch {epoch+1}/{EPOCHS} ---")

    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)
    print(f"Epoch {epoch+1} Training   - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}")

    test_loss, test_acc = evaluate(model, test_loader, criterion, DEVICE)
    print(f"Epoch {epoch+1} Validation - Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}")

print("Finished Training.")
```

---

- ë„ˆë¬´ë‚˜ë„ ë§ì´ ì°¸ê³ í•œ [ë¸”ë¡œê·¸](https://velog.io/@vantaa89/PyTorch%EB%A1%9C-Vision-Transformer-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0)