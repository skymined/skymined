> 260128 버전  
> hand-object manipulation 연구를 시작하기 전에 던져진 질문들 중 몇 가지에 대한 대답 아카이빙

다른 것들은 제외하고 hand-object manipulation의 개괄에 해당한다고 생각하는 질문 3개에 대한 대답을 모아봤다.

1\. 가상에서 물리적 hand-object manipulation 하는 연구에서 hand tracking-based와 Controller-based의 차이점과 장단점, 연구할 때의 challenge 및 연구 포인트는 무엇일까?

2\. Physics-based animation과 Kinematic-based animation의 차이점과 각각의 장단점, 해결해야 할 포인트는 무엇일까? 두 animation을 생성하는 방법의 대표적인 학습 방법론은 무엇일까?

3\. User가 실시간 control signal을 입력하는 hand manipulation method와 user controllable하지 않은 goal-based hand manipulation method(주어진 문제를 푸는 손 로봇 같은 것)의 가장 큰 차이가 무엇일까? 해당 연구를 각 agent에게 주어지는 목표를 중심을 이해해보자.

## 1.

> 가상에서 물리적 hand-object manipulation 하는 연구에서 hand tracking-based와 Controller-based의 차이점과 장단점, 연구할 때의 challenge 및 연구 포인트는 무엇일까?

### Hand traking-based

손 추적 기반 조작은 사용자의 실제 손 움직임을 가상 세계에 그대로 반영하기에 직관적이고 직접적인 상호작용이 가능하다. 신체 자체가 인터페이스이기 때문에 사용자는 컨트롤러를 사용할 필요가 없고, 터치 기반 인터렉션에서는 컨트롤러보다 정확도가 높다는 연구도 있다.

그러나 실제 손은 공중에서 가상 물체를 통과할 수 없다는 문제가 있다. 물리적 저항이 없기 때문에, 아무런 처리 없이 사용하게 된다면 가상 손이 물체를 그냥 유령처럼 통과할 수 있다. 더욱이 촉각 피드백이 없기 때문에 사용자가 얼마나 힘을 주고 있는지 판단하기가 어렵다. 이를 해결하기 위해 penetration의 정도를 힘으로 치환하는 방법이 사용되었으나, 사용자가 그 강도를 조절해야 한다는 점에서 피로도가 높아 보이므로 다른 방식을 생각할 필요가 있다.

기존 연구의 경우 가상 손이 물체와 접촉하는 순간 가상 손을 그 위치에 고정(freeze)시킨다. 실제 손이 더 움직이더라도 가상 손은 멈추기 때문에 손이 물체 안으로 들어가는 것을 방지한다. 그러나 이 경우 손가락별, 물체별로 매우 세심한 보정을 요구한다. 더욱이 손가락 별로 같은 힘이 들어간다는 것을 상정하기 때문에 물체를 쥐었을 경우 엄지 손가락이 나머지 네 손가락과 평형을 이루지 않아 진동이 발생한다. (jitter이나 pop out 현상) 더욱이 손 등으로 센서를 가릴 경우 인식이 끊기도 작은 물체나 복잡한 손 제스처에서 오류가 발생하기 쉽다.

이에 하이브리드 시스템이 연구되기도 한다. 예를 들어 큰 동작은 손 추적으로 처리하되, 물체에 손이 가까워지면 미묘하게 자석처럼 끌어당기는 기법을 사용하거나, 손가락이나 손등에 작은 진동 모터를 부탁해서 가상 접촉이 발생하면 해당 위치에 진동을 발생시키는 등 Vibrotactile haptics를 구현하는 방식이다.

**도전과제**

손 추적 데이터의 품질이 낮아 정확한 관절 위치를 추정하기가 어렵다. 손가락이 물체 안으로 관통되는 penetration 현상을 막기 위해 빠른 충돌 검사와 반력 모델링이 필요하다. 그리고 손 추적만으로는 힘을 측정할 수 없기 때문에 허공에 달린 가상 손이 물체를 잡는 힘을 어떻게 제공할지 연구해야 한다. \`Controllers or Bare Hands?\`에 의하면 실험에서 참가자들이 정확한 목표 조작이나 raycast 인터랙션에서는 컨트롤러를 사용했다고 하였다.

### Controller-based

컨트롤러 기반 조작은 현재 소비자용 VR 애플리케이션에서 가장 지배적인 접근 방식이다. 사용자가 컨트롤러를 손에 쥐고 있으며, 컨트롤러는 손의 위치와 방향에 대한 추적 정보를 제공하고 트리거 버튼을 통해 입력한다. 이때 사용자는 컨트롤러를 쥐고 있어야 하며 다섯 개의 손가락을 독립적으로 움직이기 어렵기 때문에 자연스럽지 않고 불편함이 있을 수 있다. 대신 손목의 위치나 사용자가 물체를 잡고 싶은지에 대한 의도 반영을 확실하게 할 수 있다는 점에서는 편리하고 사용자가 느끼는 피로도가 적다. 결국 사용자가 어떤 것을 더 편하게 여길지에 따라서 두 방식 중 하나를 선택해야 할 것 같다.

#### 1\. Attachment 기반 방법

잡기 동작이 시작될 때 물체를 손에 그냥 부착하는 방식으로 물리 계산이 매우 단순화되었다. 사용자가 잡기 버튼을 누르면 가상 손가락이 물체를 감싸는 애니메이션을 수행하고, 접촉이 발생하는 순간 물체는 손에 고정된다. 이 접근은 본질적으로 kinematic 방식이며 힘이나 무게보다는 안정성과 단순성이 우선된다. 눌러서 잡고, 놓아서 떨어뜨리는 방식만 익히면 되므로 학습 부담이 없고 동작이 간단하여 예측이 가능하다. 잡고 있는 동안에는 복잡한 물리 시뮬레이션을 수행하지 않으므로 계산도 간단하다.

하지만 그만큼 물리적 현실성이 부족하다는 단점이 있다. 물체가 사실상 손에 용접된 것처럼 붙어 있기 때문에 무게나 관성에 대한 감각이 전혀 없다. 사용자는 잡는 힘을 느슨하게 조절할 수 없으며, 버튼을 완전히 놓기 전까지는 물체가 미끄러지지 않는다. 사용자는 집게 잡기(pinch)나 파워 그림(power grip) 중 하나를 선택할 수 없다. 현실성은 포기하고 편의성과 안정성을 선택한 접근이다.

그러나 현실성을 포기한 것이 사용자에게 불편함을 준다고는 할 수 없을 것 같다. 오히려 한 번 잡으면 트리거를 누르고만 있으면 된다는 점에서 게임을 하는 사용자에게 있어서는 더 나은 방법이다.

**도전 과제**

-   안정성을 희생하지 않으면서도 일정 수준의 현실성을 추가하는 것.
-   하이브리드 방법: 물체를 손에 부착하되, 회전이나 휘두르는 동작에 대해서는 간단한 물리 모델을 적용하여 물체가 손 안에서 회전하거나 흔들리도록 만들어 무게감을 모사함.
-   Adaptive grip animations: 물체의 형태에 따라 서로 다른 애니메이션이나 손가락 자세를 선택. 예를 들어 작은 물체에는 집게 잡기, 큰 물체는 주먹 쥐기.(grasp synthesis)
-   Value index: 개별 손가락의 움직임을 감지할 수 있는 손가락 추적 컨트롤러를 사용
-   Attachment 상태로의 전환과 해제를 어떻게 부드럽게 처리할 것인가.

#### 2\. Torque-Based 방법

사용자가 트리거를 누를 때, 시스템이 이를 Grip Force로 해석하고, 그에 대응하는 토크를 가상 손의 손가락 관절에 적용하는 방식이다. 가상 손가락들은 이러한 힘으로 물체를 밀어 누르며, 물체는 마찰력과 접촉력에 의해서 손 안에 유지된다. 사용자가 손을 휘두를 경우 물체의 관성으로 인해 미끄러질 수 있는데 이는 물리적 개연성을 위함이다. VR-HandNet(TVCG 2023)은 DRL 기반으로, 모션 캡처 데이터를 사용해 인간의 손이 서로 다른 형태의 물체를 자연스럽게 감싸는 방식을 학습했다. 그 결과 물체의 형태와 사용자의 트리거 입력이 주어지면, 손가락을 현실적으로 닫아 물체를 잡을 수 있도록 적절한 관절 토크를 출력하는 policy가 만들어졌다. 이에 따라 얇은 물체를 잡을 때는 두 손가락이 더 가까이 모이는데 이는 별도의 애니메이션을 만든 것이 아니라 학습된 정책으로 인함이다. 그러나 이 방법은 명시적인 힘 정보가 없는 데이터셋에서 동작을 추론하였기 때문에 올바른 손가락 배치를 만들 수는 있었으나 잡기 강도의 변화를 정확하게 시뮬레이션하지는 못했다고 한다. 이를 바탕으로 제안된 ForceGrip(SIGGRAPH 2025)의 경우 모션 캡쳐 데이터에 의존하지 않고 물체의 무게와 형태, 사용자의 입력을 무작위로 조합한 학습 시나리오를 절차적으로(curriculum learnng) 생성하여 학습을 진행했다. 그 결과 사용자가 의도한 잡기 힘을 컨트롤러에 반영할 수 있었다.

하지만 Torque 기반 방법은 복잡하기 때문에 비싸다. Isaac Gym 같은 최신 시뮬레이터을 이용한다 해도 GPU 상에서 수천 개의 물리 인스턴스를 병렬로 실행할 수 있음에도 많은 시간을 투자해야 한다. 더욱이 물리 엔진은 갑자기 관통하듯이 튀어오르거나 비정상적인 동작을 보일 수 있는데 이는 시뮬레이션 자체의 안정성 문제다. 더욱이 현실성이 높아진다고 해서 사용자 경험이 향상되는 것은 아니기 때문에 사용자는 해당 방법을 이용했을 때 더 어렵고 피로하다고 느낄 수 있다.

**도전과제**

ForceGrip은 커리큘럼 학습을 통해 빠른 수렴을 시도했지만 그럼에도 많은 수의 시도가 필요하다. 이에 학습 샘플 효율성을 높여야 한다. 이것은 컨트롤러의 한계지만, 다섯 손가락을 자유자재로 움직이기가 힘들다. 그렇다면 사용자의 일반적인 의도에 맞는 손가락의 움직임이 필요할지도 모른다. 미세한 입력 신호에 맞춰 고차원 레벨로 확장시켜야 하는데, 그 미세한 컨트롤이 사용자의 선호에 맞을지가 미지수이므로 균형을 잘 맞추는 것이 중요해 보인다. 아래 사진처럼 다양한 grip을 실제로 실천할 수 있는 컨트롤러도 만들어지는 것 같다.

![](https://blog.kakaocdn.net/dna/U45G2/dJMcaaD7koE/AAAAAAAAAAAAAAAAAAAAACv65bj7G1UuUTs9jiDH-tY-vtEbjnZ_GGwp1IO0d9la/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=QlWEEWoU7GFWfpEEOP1Z3JKv2yM%3D)

## 2

> Physics-based animation과 Kinematic-based animation의 차이점과 각각의 장단점, 해결해야 할 포인트는 무엇일까? 두 animation을 생성하는 방법의 대표적인 학습 방법론은 무엇일까?

Kinematic-based animation은 물리 법칙을 직접 푸는 것이 아니라 데이터 상에 존재하는 포즈나 트랙들을 기하학적으로 계산하거나 데이터를 기반으로 움직임을 만드는 것이고, Physics-based animation은 질량, 관성, 접촉, 마찰을 포함한 실제 물리법칙들을 실제로 시뮬레이션 상에 구현한 것이다. 그래서 힘이나 토크를 제어해서 움직임을 만드는 것이다.

### Kinematic-based animation

물리 법칙을 풀지 않고 관절의 각도, 포즈, root trajectory를 직접 만들어내는 방식 전체를 말하며 결과물이 pose sequence면 kinematic이라고 한다.

-   장점: 같은 입력을 넣었을 때 같은 동작이 나오므로 안정적이고 예측가능하다. motion capture 데이터를 그래도 사용할 수 있으므로 지도학습이 가능하다는 면에서 데이터 친화적이라고 할 수 있다. 게임/VR에서 즉각적인 반응이 되므로 실시간 제어에 강하고 반드시 학습이 필요하지 않다는 장점이 있다.
-   단점: 그러나 물리 법칙을 실제로 적용시킨 것이 아니기 때문에 접촉이나 마찰과 같은 현상을 자연스럽게 만들어내기가 어렵다. 마찬가지로 계단과 같은 경사가 있는 지형이나 충돌이 일어났을 경우에 적합한 결과가 나타나지 않는다. 실제로 data-driven으로 학습시킨 캐릭터의 경우 발이 질질 끌리거나 공중에 뜨는 현상이 자주 일어난다.(SMPL의 경우) 결국 데이터를 가지고 만드는 것이기 때문에 데이터의 범위를 벗어난 상황이 벌어지면 쉽게 깨진다는 단점이 있다. (=일반화 부족)  
    때문에 이 방법론에서는 발 미끄러짐과 같은 것들을 제거하고(접촉 일관성) 프레임 간의 떨림이나 불연속적인 상황을 방지하고 새로운 환경/목표에 대한 적응이 필요하다.

<생성방법>  
매 프레임마다 pose, joint angle, root trajectory를 직접 만들어야 한다. 대표적으로는 Motion Matching이 있는데, 이는 현재 상태와 '원하는 가까운 미래 궤적'을 특정 벡터로 만든 다음, 데이터베이스에서 가장 잘 맞는 프레임을 검색하여 선택한 프레임을 현재 프레임으로 사용하는 것을 말한다. 지도 학습이 없어도 자연스러운 결과가 나오고, 정답을 처음부터 예측하는 것이 아니라 이미 검증된 모션을 가지고 오는 문제로 바꿨기 때문에(일종의 근사) 비교적 쉽게 성능 달성이 가능하다.

### Physics-based animation

캐릭터와 환경을 물리 시스템으로 두고, 힘이나 토크를 제어해서 애니메이션 결과를 얻는 방식이다.

-   장점: 물리 법칙에 따라 외부에서 주는 힘에 반응하며 설계가 잘 되었다면 자연스러운 움직임을 낼 수 있다. 물리 모델과 설계만으로 다양한 행동을 생성할 수 있기 때문에 데이터 수집 비용 자체는 적다. 더욱이 물체 무게나 마찰 변화에도 대응이 가능하다. (일반화 성능 높음)
-   단점: 하지만 Kinematic 기반과는 달리 단순히 값을 조정하여 원하는 결과를 내기가 어렵고 결과를 예측하기가 힘들다.(그렇다고 kinematic 기반이 쉬운 것 같지는 않다.) 목표 동작을 얻기 위해서 단순히 신체 값을 조정하는 것이 아니라 제약 조건과 제어 알고리즘을 수정하기 때문이다. 물리 엔진에서 충돌과 접촉과 같은 물리 시스템을 계산해야 하기 때문에 시뮬레이션 비용도 높다. 거기다 본질적으로 Action -> Result의 관계가 불명확하고 정답 라벨이 없기 때문에 학습 디버깅이 어렵다.  
    Physics-based animation에서 반드시 해결해야 하는 문제는 안정성이다. 단순히 한 프레임에서 균형을 유지하는 것이 아니라, 시간에 걸쳐 전체 trajectory가 붕괴하지 않는 것을 의미한다. 물리 시스템에서는 작은 오차 하나가 관성이나 접촉력, 중력과 같은 힘과 결합되어 수 프레임 뒤에 큰 실패로 이어질 수 있다. 때문에 animation(human 가정)에서는 넘어지지 않는 것이 절대 조건이다. 더욱이 접촉에 대한 문제도 생각해봐야 한다. 지면과의 접촉이나 충돌 등은 모두 불연속적이며 미분 불가능한 사건이다. 이러한 접촉에 알맞게 반응할 수 있어야만 제대로 된 animation을 만들었다고 볼 수 있다.

<생성 방법>  
매 프레임마다 torque, force를 만들고, pose는 물리 시뮬레이션의 결과로 나타낸다.  
대표적인 학습 방법론은 강화학습, 그 중에서도 PPO 계열이다. 정책이 상태를 보고 토크(A)를 출력하면 보상으로 넘어지지 않거나 목표로 이동하는 등의 보상을 얻을 수 있다. 애초에 목표 동작을 얻기 위해 개별 관절 값이 아니라 보상 함수, 제약 조건 등을 조정해야 하기 때문에 MDP로 재정의할 수 있다.정답 토크가 존재하지 않고 미분 불가능하며 성공/실패가 trajectory 단위라는 점으로 미루어보아 사실상 physic-based animaton에서 RL은 이러한 문제 정의가 요구하는 것과 일치한다.

> **Q. 왜 Physics-based는 거의 RL을 쓰는가?**  
> 물리 엔진은 일반적으로 미분 불가능하기 떄문에 정책을 학습하기 위해 RL을 사용한다. (e.g. collision, friction, slip...등)Physics-based는 토크 시퀀스가 필요한데, 데이터에는 정답 토크가 없기 때문이다. (= 지도 학습 불가능)성공 조건이 단순히 한 프레임이 아니라 trajectory level이기 때문에 누적 보상으로 표현해야 하기 때문이다. 예를 들어 넘어지지 않는다던가 균형을 유지한다던가 하는 것들은 한 프레임으로는 성공 여부를 알 수가 없다.  
> 꼭 둘 중 하나로만 사용해야 하는 것은 아니다. physics-based으로만 단독학습할 경우 공간이 고차원이고 접촉은 불연속이고 넘어지면 episode가 종료된기 때문에 아무런 지도 없이 그냥 RL만 돌리면 대부분의 시도가 실패하게 된다. 실제로는 참조 모션을 가지고 오고, physics는 토크를 출력해 해당 모션을 물리적으로 따라가게 하거나(Reference Motion + Physics Traking), Kinematic이 다음 발을 디딜 위치나 root trajectory를 출력하면 physics는 토크로 실제 실행을(High-level kinematic + Low level physics) 하는 등의 방법을 사용한다. (learning to use chopsticks in diverse gripping styles(siggraph2022)는 고수준을 kinematic motion planner로, 저수준을 physics-based RL controller로 사용하였음)  
> 그러나 정답이 조금도 존재하지 않거나(kinematic은 최소한 정답 pose, trajectory, 스타일 중 하나는 있어야 함) 접촉/충돌/마찰 등의 비연속적인 특징들이 핵심인 경우(IK는 contact force를 다룰 수 없음) RL이 유일한 선택지이기 때문에 이를 사용해야 한다.

## 3

> User가 실시간 control signal을 입력하는 hand manipulation method와 user controllable하지 않은 goal-based hand manipulation method(주어진 문제를 푸는 손 로봇 같은 것)의 가장 큰 차이가 무엇일까? 해당 연구를 각 agent에게 주어지는 목표를 중심을 이해해보자.

### 사용자 제어 방식(User-controlled control)

사용자가 VR 컨트롤러를 조작해서 손목 위치나 트리거 입력을 제공하면, 알고리즘이 손가락 토크를 계산해서 실시간으로 물체를 잡고 움직이는 것. 에이전트의 목표는 "사용자가 제공한 손목이나 트리거 값에 따라 손가락을 움직여서 물체를 안정적으로 잡고 따라가는 것". 사용자의 의도가 곧 목표 동작이 되므로 제어의 즉시성이 장점이지만 사용자의 입력 신호(e.g. 컨트롤러의 트리거 값)는 실제 손의 복잡한 관절 움직임보다 차원이 낮고 단순하기 때문에 이를 물리적으로 그럴듯한 고차원 손 움직임으로 매핑하는 것이 과제다.

-   VR-HandNet: VR 컨트롤러의 6 DoF 위치/회전과 트리거 값(잡음의 세기)을 입력으로 받아 관절 토크를 출력.한다. 매 프레임마다 가상 손 상태, 컨트롤러 입력, 손-물체 공간 관계를 입력으로 받으면 신경망이 다음 시점의 손가락 목표 관절 각도를 결정한다. RL을 이용해 물리 동역학을 학습했고 여기에 모방 학습을 도입해서 실제 인간 동작 데이터를 모방해 현실감도 높였다.
-   ForceGrip: 손가락 관절 토크와 접촉력을 RL로 학습하고, 커리큘럼 러닝을 적용해 훈련 난이도를 점진적으로 높였다. 사용자가 트리거를 얼마나 세게 쥐었는지에 따라 손가락 힘 조절과 미세한 움직임까지 재현했다. 모션 캡처 데이터에 의존하지 않고 다양한 물체 모양과 손목 움직임, 트리거 입력 강도를 무작위로 변화시킨 가상 시나리오들을 통해 에이전트를 훈련하였다. 추가적으로 거리 기반 보상함수를 설계해 손가락 움직임의 자연스러움을 향상시켰다.

사용자 제어 손 조작 분야에서는 사용자의 의도를 최대한 왜곡 없이 전달하면서도 물리 법칙을 준수하는 상호작용을 실현하고자 한다. 최근 연구들은 단순히 손의 형태를 모방하는 것 뿐만 아니라 접촉력과 같은 물리력을 고려한 상호작용을 구현한다.

### 목표 기반 방식

로봇 손이 특정 물체를 이동하거나 조작하는 목표를 스스로 달성하는 방식이다. 입력은 주어진 목표의 물체 상태나 과제(e.g. 펜을 회전시켜라)로, 사람이 실시간 제어하지 않는다. 여기서 에이전트의 목표는 명시된 task를 성공적으로 완료하는 것으로 목표 달성 여부를 기준으로 보상을 받아 학습한다. 실시간 사용자 입력이 없으므로 에이전트는 목표를 내재적으로 해석하고 최적의 행동을 해내야 한다.

-   Dactyl(OpenAI. 2019): 여기서는 루빅스 큐브 맞추기라는 매우 어려운 과제가 주어지고, 인간의 개입 없이 로봇 손이 스스로 큐브를 회전시켜 맞추는 정책을 학습했다. DRL과 도메인 랜덤화 기법을 활용하여 시뮬레이션에서만 학습된 모델을 실제 로봇 손으로 가져왔다.
-   DexPBT(2023): 여러 개의 로봇 손가락을 지닌 로봇 팔을 이용해 regrasping, grasp-and-throw, reorientation 등의 다양한 조작 과제를 해결하는 프레임워크를 만들었다. Isaac Gym 내에서 병렬 학습을 돌렸고 학습이 어려울 수록 탐색 능력을 높이기 위해 PBT(Population-Based Trianing) 기법을 사용했다. (다수의 정책을 병렬적으로 학습시켜 진화적 탐색하도록 하는 것.)  
    대량의 시뮬레이션 데이터를 통해 복잡한 조작을 자율적으로 학습하는 손 에이전트를 만들려고 한다. 학습 효율을 높이기 위해 커리큘럼 학습이나 PBT 같은 탐색 보조 기법을 접목하는 것으로 보인다.

> **두 방식의 차이 정리**  
>   
> 1\. 목표: 실시간 제어에서는 목표가 사용자 입력에 의해 정의됨. 에이전트는 입력된 손목 위치/트리거에 따라서 손가락을 즉시 반응시켜야 하기 때문에 반응성 문제도 해결해야 한다. 반면 목표 기반 방법에서는 명시적인 목표(e.g. 물체를 잡고 회전해라, 방해물을 잡아서 목표 위치에 배치해라...)가 주어지며, 에이전트는 자율적으로 최적 policy를 탐색하면 된다.  
> 2\. 행동 자유도: 실시간 제어는 사용자의 명령에 따라 동작을 수행해야 하고 일치성을 중요하기 보기 때문에 에이전트의 자기 결정 범위가 제한된다. 반면 목표 기반 방법은 목표에 따라 에이전트가 최적 행동을 스스로 결정하기 때문에 탐색 범위가 크다. 때문에 효율적 학습을 위해 reward shaping이나 domain randomization이 필요하다.