{
	"nodes":[
		{"id":"60b232454ad9f10f","type":"text","text":"## **MI 접근법**\n정보이론","x":-950,"y":-375,"width":130,"height":80,"color":"4"},
		{"id":"be402cf99ae133ea","type":"text","text":"## **WDM 접근법**\n최적 수송 이론","x":-960,"y":278,"width":210,"height":90,"color":"4"},
		{"id":"9e4771ccb7e51eb4","type":"text","text":"Lipschitz-constrained Unsupervised Skill Discovery(ICLR2022) = LSD","x":-1525,"y":338,"width":356,"height":60},
		{"id":"a034c5a66a7b2fd6","type":"text","text":"Controllablity-Aware Unsupervised Skill Discovery(ICML2023) = CSD","x":-1525,"y":470,"width":360,"height":60},
		{"id":"cca61b0e3d97d4c4","type":"text","text":"[[P; METRA-Scalabel unsupervised RL with Metric-Aware Abstraction(ICLR2024)]]\nWDM을 직접 푸는 것은 불가능하므로 다른 보상함수와 같다고 증명함","x":-1525,"y":590,"width":360,"height":130,"color":"4"},
		{"id":"bf28befbd5d32b51","type":"text","text":"### **1-Lipschitz**\n입력 변화보다 출력 변화가 더 크지 않은 안정적인 함수로, 두 점의 거리보다 함수값 차이가 항상 작거나 같은 함수를 말한다\n","x":-2120,"y":241,"width":354,"height":139},
		{"id":"f3e6c811f36c665c","type":"text","text":"Wasserstein Unsupervised RL(AAAI2022)","x":-1525,"y":181,"width":356,"height":60},
		{"id":"8af9855d6c8e43b8","type":"text","text":"스킬들이 서로 구분은 잘 되지만, 에이전트가 상태 공간(State Space)을 충분히 넓게 탐색하지는 못함","x":-963,"y":-69,"width":323,"height":109},
		{"id":"3261cea066837fc9","type":"text","text":"근간: Wasserstein Dependency Measure for Representation Learning\n$$\\mathcal{I}_{W}(S; Z) \\;\\overset{\\mathrm{def}}{=}\\; \\mathcal{W}\\!\\left( P(S,Z)\\,\\big\\|\\,P(S)P(Z) \\right)\n$$\n- JSD는 분포가 조금만 떨어지면 금방 포화. \n- Wasserstein은 실제 공간적 거리만큼 증가한다. -> Exploration 유도","x":-1440,"y":-137,"width":404,"height":208},
		{"id":"8c0bca75030b22a4","type":"text","text":"**어떤 z가 원하는 스킬을 만들어낼까?**\n여러 z를 샘플링한 뒤, 각 z에 대해 수백 번의 rollout을 수행하여 행동을 평가하는 방식","x":-2147,"y":525,"width":327,"height":95},
		{"id":"78bde0d304751aeb","type":"text","text":"**왜 semantic한 상태 변화(transition)을 skill로 정의하지 않았을까?**\n1. transition을 semantic하게 알기 어려움\n\te.g. 팔이 큐브를 밀기 위해 가까워지고 있다?\n2. transition은 directinally ambiguous하다.\n\t- LLM description이 timestep마다 작은 텍스트 변화만 일어낼 수 있는데 그럼 stable하게 정의하기 힘들다\n3. LGSD의 목적은 skill use였고, 사람이 자연어로 목표를 말하면 즉시 행동하는 것이 목표였다.\n\t= skill 정의가 아니라 skill 사용 가능하게 하는 것이 목적\n4. transition을 semantic 하게 정의하면 스킬이 너무 많아진다.\n\t","x":-1920,"y":800,"width":520,"height":300,"color":"3"},
		{"id":"3f5a2964173ac980","type":"text","text":"- [ ] 멀리 가는것? 그게 어떻게 스킬임?\n-","x":-801,"y":410,"width":481,"height":150,"color":"6"},
		{"id":"7b30067648dc1458","type":"text","text":"#  **Skill Discovery**\nSkill을 어떻게 나눌까?","x":-390,"y":-137,"width":390,"height":159,"color":"5"},
		{"id":"db9c60ac07a61494","type":"text","text":"Creating Multi-Level Skill Hierarchies in Reinforcement Learning(NeurIPS 2023)","x":-320,"y":-1181,"width":740,"height":401,"color":"2"},
		{"id":"3ddc873f493c3399","type":"text","text":"DIAYN(2018)\n상태를 보면 어떤 z인지 구분 가능해야 한다.","x":-1560,"y":-435,"width":356,"height":60},
		{"id":"49286752d70067bc","type":"text","text":"[[P; Dynamics-aware Unsupervised Discovery of Skills(ICLR 2020)]]\n좌표 변화, 속도 변화, 가는 방향 = Skill이다","x":-1656,"y":-335,"width":548,"height":60},
		{"id":"97336b300c2da899","type":"text","text":"## **VLM을 사용**","x":480,"y":-87,"width":240,"height":60,"color":"4"},
		{"id":"4ba6d294074347df","type":"text","text":"##### 의문점들\n- sasasa 말고 다른 방식으로 스킬을 만들 수는 없을까?\n- 스킬은 항상 먼저 최대한 많이 찾은 다음, 의미 없는 걸 제거해야 하나? 다른 방식은 없나?\n- context를 스킬 learning 넣지 못하나?","x":-195,"y":-634,"width":497,"height":149,"color":"6"},
		{"id":"58e67626c3640f09","type":"text","text":"### **VLM을 이용해 Reward를**","x":965,"y":-386,"width":335,"height":60},
		{"id":"68ff04795b12ccab","type":"text","text":"Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning(ICLR2024) = VLM-RM","x":1360,"y":-559,"width":510,"height":60},
		{"id":"5b52e9d0e98bd3b7","type":"text","text":"**GoldLadder**\nVLM을 이용해 에이전트가 도달 가능한 '중간 목표'를 발견하고 사다리를 놓듯 연결한다.","x":1400,"y":-433,"width":509,"height":77},
		{"id":"f0b589152e93c663","type":"text","text":"## **Skill DIscovery의 ","x":627,"y":-790,"width":250,"height":60},
		{"id":"ed386a152ece3818","type":"text","text":"VanTA: Semantic Temporal Abstraction via Vision-Language Model Guidance for Efficient Reinforcement Learning (ICLR 2026)","x":1035,"y":60,"width":586,"height":80},
		{"id":"66b0e95e895f0bd4","type":"text","text":"# **어떤 z가 어떤 행동을 만들어낼까?**","x":-507,"y":40,"width":265,"height":100},
		{"id":"418e39615ac0be31","type":"text","text":"**Unsupervised Reinforcement Learning with Contrastive Intrinsic Control(NeurIPS 2022)**\n스킬 공간을 일정한 간격으로 전부 스캔(sweep)하는 방식","x":-609,"y":210,"width":414,"height":137},
		{"id":"70e5b3af0a4ddc38","type":"text","text":"## **LLM 사용**","x":200,"y":410,"width":250,"height":60,"color":"4"},
		{"id":"b7ecab1bd1d2e28f","type":"text","text":"[[P; Language Guided Skill Discovery]]","x":-879,"y":640,"width":540,"height":60},
		{"id":"aea1e28f8db439bc","type":"text","text":"[[P; Language to Rewards for Robotic Skill Synthesis(CoRL 2023)]]\nGoogle D","x":-512,"y":740,"width":540,"height":106},
		{"id":"c1bea49f80e574fd","type":"text","text":"Eureka:  Human-Level Reward Design via Coding Large Language Models(ICLR 2024)\n- NVIDIA\n- github link: https://github.com/eureka-research/Eureka","x":720,"y":670,"width":563,"height":141},
		{"id":"f955b958c87786f2","type":"text","text":"Guiding Pretraining in Reinforcement Learning with Large Language Models(ICML 2023)\n- ELLM: 에이전트의 현재 상태를 설명하는 프롬프트를 기반으로 언어 모델이 제안한 목표를 달성했을 때 에이전트한테 보상을 제공함","x":157,"y":780,"width":563,"height":131},
		{"id":"0ecce9ade852fc93","type":"text","text":"**Agentic Skill DIscovery**\nLLM을 이용해서 점진적으로 나아갈 수 있게 함.","x":1136,"y":440,"width":384,"height":60},
		{"id":"9061e7e32eba3907","type":"text","text":"## Reward 생성\n","x":1866,"y":102,"width":250,"height":60},
		{"id":"050f34ee7aacb127","type":"text","text":"**Video2Reward**","x":1872,"y":389,"width":250,"height":60},
		{"id":"579239406a35bfc7","type":"text","text":"###### VLM-RM(ICLR 2024)","x":1440,"y":-300,"width":469,"height":60},
		{"id":"142e39ef579cdc54","type":"text","text":"###### RL-VLM-F()","x":1437,"y":-177,"width":250,"height":60},
		{"id":"e51acc3973bfaf33","type":"text","text":"$$\\mathcal{I}(S; Z) \\;\\overset{\\mathrm{def}}{=}\\; D_{KL}\\left( P(S,Z)\\,\\big\\|\\,P(S)P(Z) \\right)\n$$\n- 초기에 MI를 사용한 이유: 멀리 가는 것보다는 서로 다른 행동을 하는 것이었기 때문. JSD는 분포가 겹치는지 안겹치는지 판단하기에는 아주 효율적이고 표준적","x":-1180,"y":-830,"width":360,"height":200}
	],
	"edges":[
		{"id":"93bc33c18ed546e4","fromNode":"7b30067648dc1458","fromSide":"top","toNode":"60b232454ad9f10f","toSide":"right"},
		{"id":"39f6033dcbf26fb2","fromNode":"be402cf99ae133ea","fromSide":"left","toNode":"f3e6c811f36c665c","toSide":"right"},
		{"id":"0cfb3e9d69d822cc","fromNode":"be402cf99ae133ea","fromSide":"left","toNode":"9e4771ccb7e51eb4","toSide":"right"},
		{"id":"23d46858357f18ee","fromNode":"60b232454ad9f10f","fromSide":"left","toNode":"3ddc873f493c3399","toSide":"right"},
		{"id":"f7c03d7933044c04","fromNode":"7b30067648dc1458","fromSide":"left","toNode":"be402cf99ae133ea","toSide":"right"},
		{"id":"3c1d171b5a57a51c","fromNode":"be402cf99ae133ea","fromSide":"left","toNode":"a034c5a66a7b2fd6","toSide":"right"},
		{"id":"5e2f0dff3e00d1e6","fromNode":"be402cf99ae133ea","fromSide":"top","toNode":"3261cea066837fc9","toSide":"bottom"},
		{"id":"b0b468c354237790","fromNode":"60b232454ad9f10f","fromSide":"top","toNode":"e51acc3973bfaf33","toSide":"bottom"},
		{"id":"89e9157eca896de2","fromNode":"7b30067648dc1458","fromSide":"bottom","toNode":"70e5b3af0a4ddc38","toSide":"top"},
		{"id":"7d9326c105ed6445","fromNode":"70e5b3af0a4ddc38","fromSide":"bottom","toNode":"aea1e28f8db439bc","toSide":"top"},
		{"id":"103a1174ec625f65","fromNode":"60b232454ad9f10f","fromSide":"left","toNode":"49286752d70067bc","toSide":"right"},
		{"id":"bc4f278c8d2b05bc","fromNode":"70e5b3af0a4ddc38","fromSide":"bottom","toNode":"c1bea49f80e574fd","toSide":"top"},
		{"id":"9083a3a50220ea33","fromNode":"70e5b3af0a4ddc38","fromSide":"bottom","toNode":"f955b958c87786f2","toSide":"top"},
		{"id":"ee373881f76917b3","fromNode":"be402cf99ae133ea","fromSide":"left","toNode":"cca61b0e3d97d4c4","toSide":"right"},
		{"id":"44baec3685b39117","fromNode":"cca61b0e3d97d4c4","fromSide":"left","toNode":"9e4771ccb7e51eb4","toSide":"left","label":"박서홍\n(https://seohong.me/)"},
		{"id":"a8f42f6ffe17f0e2","fromNode":"70e5b3af0a4ddc38","fromSide":"left","toNode":"b7ecab1bd1d2e28f","toSide":"right"},
		{"id":"75076351fba7a241","fromNode":"be402cf99ae133ea","fromSide":"bottom","toNode":"b7ecab1bd1d2e28f","toSide":"top"},
		{"id":"bbbf4f35629d041b","fromNode":"60b232454ad9f10f","fromSide":"bottom","toNode":"8af9855d6c8e43b8","toSide":"top"},
		{"id":"5e7247a97d67e907","fromNode":"8af9855d6c8e43b8","fromSide":"bottom","toNode":"be402cf99ae133ea","toSide":"top"},
		{"id":"d8864e44b5210085","fromNode":"7b30067648dc1458","fromSide":"bottom","toNode":"66b0e95e895f0bd4","toSide":"top"},
		{"id":"dd6cde5682766544","fromNode":"66b0e95e895f0bd4","fromSide":"bottom","toNode":"418e39615ac0be31","toSide":"top"},
		{"id":"6b8ce8cd52474682","fromNode":"a034c5a66a7b2fd6","fromSide":"left","toNode":"8c0bca75030b22a4","toSide":"right"},
		{"id":"48b17cddbaf7d3c5","fromNode":"b7ecab1bd1d2e28f","fromSide":"left","toNode":"78bde0d304751aeb","toSide":"right"},
		{"id":"48fb01e2a5c35267","fromNode":"7b30067648dc1458","fromSide":"right","toNode":"97336b300c2da899","toSide":"left"},
		{"id":"8ee1a00c44095594","fromNode":"58e67626c3640f09","fromSide":"right","toNode":"68ff04795b12ccab","toSide":"left"},
		{"id":"4a2dc48ea5e88d89","fromNode":"58e67626c3640f09","fromSide":"right","toNode":"5b52e9d0e98bd3b7","toSide":"left"},
		{"id":"a2334ec3e9499ee9","fromNode":"97336b300c2da899","fromSide":"right","toNode":"58e67626c3640f09","toSide":"left"},
		{"id":"c4bbae04ed236423","fromNode":"97336b300c2da899","fromSide":"bottom","toNode":"ed386a152ece3818","toSide":"left"},
		{"id":"2bb294b1354b770b","fromNode":"97336b300c2da899","fromSide":"top","toNode":"f0b589152e93c663","toSide":"bottom"},
		{"id":"e8a7c149f49e9a5c","fromNode":"70e5b3af0a4ddc38","fromSide":"right","toNode":"0ecce9ade852fc93","toSide":"left"},
		{"id":"76c65d2c08e89c9e","fromNode":"9061e7e32eba3907","fromSide":"bottom","toNode":"050f34ee7aacb127","toSide":"top"},
		{"id":"6fb41b3669a0a106","fromNode":"58e67626c3640f09","fromSide":"right","toNode":"579239406a35bfc7","toSide":"left"},
		{"id":"b463af61b8dedda8","fromNode":"58e67626c3640f09","fromSide":"right","toNode":"142e39ef579cdc54","toSide":"left"}
	]
}